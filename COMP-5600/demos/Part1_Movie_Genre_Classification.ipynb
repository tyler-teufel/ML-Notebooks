
# Part 1: Movie Genre Classification

## Import Libraries
```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
import os
```

## Data Loading and Preparation
```python
# Load dataset
# Replace this with the actual path to the dataset
data = pd.read_csv("IMDB-Movie-Data.csv")
print(data.head())

# Select relevant columns
data = data[['Title', 'Genre', 'Description']]
data = data.dropna()

# Process genres into multi-label format
mlb = MultiLabelBinarizer()
data['Genre'] = data['Genre'].apply(lambda x: x.split(','))
labels = mlb.fit_transform(data['Genre'])
print(mlb.classes_)

# Clean descriptions
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(description):
    tokens = nltk.word_tokenize(description.lower())
    return ' '.join([lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words])

data['Description'] = data['Description'].apply(clean_text)
print(data['Description'].head())
```

## Embedding and Tokenization
```python
# Tokenize and pad sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['Description'])
sequences = tokenizer.texts_to_sequences(data['Description'])
word_index = tokenizer.word_index
print(f"Found {len(word_index)} unique tokens.")

max_len = 100  # Define max length for padding
X = pad_sequences(sequences, maxlen=max_len)
y = labels

# Train-validation-test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
```

## Model 1: RNN
```python
model_rnn = Sequential([
    Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=max_len),
    SimpleRNN(128, return_sequences=False),
    Dense(20, activation='sigmoid')
])

model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.summary()

history_rnn = model_rnn.fit(
    X_train, y_train, 
    validation_data=(X_val, y_val), 
    epochs=20, batch_size=32
)
```

## Model 2: LSTM
```python
model_lstm = Sequential([
    Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=max_len),
    LSTM(128, return_sequences=False),
    Dense(20, activation='sigmoid')
])

model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_lstm.summary()

history_lstm = model_lstm.fit(
    X_train, y_train, 
    validation_data=(X_val, y_val), 
    epochs=20, batch_size=32
)
```

## Evaluation
```python
# Plot training/validation metrics
def plot_metrics(history, model_name):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title(f'{model_name} Loss')
    plt.legend()
    plt.show()

plot_metrics(history_rnn, "RNN")
plot_metrics(history_lstm, "LSTM")
```

