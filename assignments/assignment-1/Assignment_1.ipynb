{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh4jzREgaEx5"
      },
      "source": [
        "# Question 1\n",
        "\n",
        "Suppose that you are conducting a scientific experiment where you are observing the effects of one\n",
        "variable (x_train.npy and x_test.npy) on the output (y_train.npy and y_test.npy).\n",
        "\n",
        "<br>\n",
        "\n",
        "Your goal is to come up with a linear regression model that can take the training data (x_train.npy and\n",
        "y_train.npy) and model the relationship between the variables x and y. You should implement your\n",
        "own version of linear regression either using gradient descent or normal equations. You SHOULD\n",
        "NOT use any pre-packaged library such as Sci-Kit Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "h3AtD5c1GbJA",
        "outputId": "ccb6e30e-e831-449c-c26e-478c745dd1d5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def mse(true_values, predicted_values):\n",
        "    return np.mean((true_values - predicted_values) ** 2)\n",
        "\n",
        "\n",
        "def poly_features(X, degree):\n",
        "    out = X.copy()\n",
        "    for i in range(2, degree + 1):\n",
        "        out = np.c_[out, X ** i]\n",
        "    return out\n",
        "\n",
        "\n",
        "# Load in training data from google drive.\n",
        "\n",
        "# xtrain = np.load('/content/drive/MyDrive/COMP-5630/x_train.npy')\n",
        "# ytrain = np.load('/content/drive/MyDrive/COMP-5630/y_train.npy')\n",
        "\n",
        "# Load in training data locally.\n",
        "\n",
        "xtrain = np.load('x_train.npy')\n",
        "ytrain = np.load('y_train.npy')\n",
        "\n",
        "\n",
        "# Load in test data from google drive.\n",
        "# xtest = np.load('/content/drive/MyDrive/COMP-5630/x_test.npy')\n",
        "# ytest = np.load('/content/drive/MyDrive/COMP-5630/y_test.npy')\n",
        "\n",
        "# Load in test data locally.\n",
        "xtest = np.load('x_test.npy')\n",
        "ytest = np.load('y_test.npy')\n",
        "\n",
        "# Add the column of ones\n",
        "# According to numpy docs, '.c_' functions are used to\n",
        "# translate slice objects to concatenation along the second axis.\n",
        "X = np.c_[np.ones(xtrain.shape[0]), xtrain]\n",
        "\n",
        "# Apply the normal equation to find the optimal parameters\n",
        "# The product of the inverse of X transposed multiplied by X,\n",
        "# and X transposed multiplied by the vector ytrain\n",
        "theta_vals = np.linalg.inv(X.T @ X) @ X.T @ ytrain\n",
        "\n",
        "\n",
        "# Since we added the first column of 1s, doing matrix multiplication\n",
        "# with the X matrix and the vector containing Theta_0 (bias or y-intercept)\n",
        "# and Theta_1 (slope or weight).\n",
        "ypred = X @ theta_vals\n",
        "\n",
        "\n",
        "# Generate mse values.\n",
        "mse_train = mse(ytrain, ypred)\n",
        "mse_test = mse(ytest, ypred)\n",
        "\n",
        "print(\"MSE on Training Data:\", mse_train)\n",
        "print(\"MSE on Test Data:\", mse_test)\n",
        "\n",
        "\n",
        "# Generate a graph containing the test data, training data, and the linear\n",
        "# fitted line based on the predictions that utilize the theta values.\n",
        "plt.scatter(xtrain, ytrain, color='blue', label='Training Data')\n",
        "plt.scatter(xtest, ytest, color='green', label='Test Data')\n",
        "plt.plot(xtrain, ypred, color='red', label='Fitted Line')\n",
        "plt.xlabel('x_test')\n",
        "plt.ylabel('y_test')\n",
        "plt.title('Scatter Plot of Test Data')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# *The xtrain and xtest features are run through the poly function.* #\n",
        "######################################################################\n",
        "\n",
        "\n",
        "# Generate polynomial features for the normalized data.\n",
        "# Degree three will make it a cubic function.\n",
        "xtrain_poly = poly_features(xtrain, 3)\n",
        "xtest_poly = poly_features(xtest, 3)\n",
        "\n",
        "\n",
        "# Add bias term (column of ones).\n",
        "X_poly = np.c_[np.ones(xtrain_poly.shape[0]), xtrain_poly]\n",
        "\n",
        "# Fit using normal equation.\n",
        "theta_poly = np.linalg.inv(X_poly.T @ X_poly) @ X_poly.T @ ytrain\n",
        "\n",
        "# Make predictions on the y values, and adjust the x values for bias.\n",
        "ypred_poly_train = X_poly @ theta_poly\n",
        "X_poly_test = np.c_[np.ones(xtest_poly.shape[0]), xtest_poly]\n",
        "ypred_poly_test = X_poly_test @ theta_poly\n",
        "\n",
        "\n",
        "# Calculate MSE for the training and test sets\n",
        "mse_train_poly = mse(ytrain, ypred_poly_train)\n",
        "mse_test_poly = mse(ytest, ypred_poly_test)\n",
        "\n",
        "print(\"MSE on Training Data (Polynomial):\", mse_train_poly)\n",
        "print(\"MSE on Test Data (Polynomial):\", mse_test_poly)\n",
        "\n",
        "# Generate a graph containing the test data, training data, and the polynomial\n",
        "# fitted line based on the predictions that utilize the theta values.\n",
        "# The purpose of '.linspace' is to return evenly spaced values over an interval.\n",
        "x_plot = np.linspace(min(xtrain), max(xtrain), 100)\n",
        "x_plot_poly = poly_features(x_plot, 3)\n",
        "X_plot_poly = np.c_[np.ones(x_plot_poly.shape[0]), x_plot_poly]\n",
        "y_plot = X_plot_poly @ theta_poly\n",
        "\n",
        "# Plotting initial cubic function\n",
        "plt.scatter(xtrain, ytrain, color='blue', label='Training Data')\n",
        "plt.scatter(xtest, ytest, color='green', label='Test Data')\n",
        "plt.plot(x_plot, y_plot, color='red', label='Polynomial Fit')\n",
        "plt.xlabel('x_test')\n",
        "plt.ylabel('y_test')\n",
        "plt.title('Scatter Plot with Polynomial Fit')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZiI8Jbd5ti2"
      },
      "source": [
        "# Question 1 Report\n",
        "\n",
        "<br>\n",
        "\n",
        "### a) What kind of relationship exists between the variables (linear, non-linear, or something else)?\n",
        "\n",
        "<br>\n",
        "The relationship that exists here between the two variables is most definitely nonlinear, as the graph of the training data most closely represents the sigmoid function. As you can see by the calculated Mean Squared Error (MSE), which is large even by the standards of the range of the training data, a linear representation realistically is not the best choice in this situation, due to the fact that it is indeed a nonlinear relationship.\n",
        "\n",
        "<br>\n",
        "\n",
        "### b) Does the current model need improvements to capture the observed relationship effectively?\n",
        "- If so, what modifications would you suggest to better capture the data pattern?\n",
        "- What types of transformations or additional features might improve the model's performance?\n",
        "After implementing these changes, how does each new feature or transformation impact the fit of the model?\n",
        "Plot and analyze them individually to assess the impact.\n",
        "\n",
        "<br>\n",
        "\n",
        "The current model would absolutely benefit from adding in polynomial features, as from the look of it, the graph most closesly represents a cubic function. Perhaps creating a function that manually adds higher degree terms for the input features could generate a line that better fits the graph data. I demonstrated this above, where you can see from both the graph as well as the updated MSE that the new line is a significantly better fit. The transformation that took place essentially shows a far more fitting relationship in terms of how the two variables impact eachother. The relationship as x increases clearly shows a trend resembling a cubic function for the training data, with the data between the relative minima and maxima also somewhat covering the test data as well. Overall, the impact displayed is tremendous, which with more data can undoubtedly be refined such that it is an optimal fit.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# Report\n",
        "\n",
        "<br>\n",
        "\n",
        "To break down the code line by line, lets start with the beginning, where the program begins by first defining two functions to be used throughout- the Mean Squared Error function, which simply is a helper function to quick implement the computation of the total MSE for the data at each point, as well as the polynomial adjustment function, which first creates a copy of the design Matrix, X, and then iterates from degree 2 to the inputed end polynomial degree, adding those columns for the values adjusted.\n",
        "\n",
        "<br>\n",
        "\n",
        "After these helper functions, we load in the data from the 4 different sets of input  files, and then proceed to create the design matrix using the training data, with a column of all 1s appended to the front for accounting for the y-intercept or bias in the next step. Next, we compute the vector of theta values, the y-intercept/bias and the slope/weight, using the normal equation. After computing this optimal theta vector, we will create a new set of y  values that are the predicted values, calculated by taking the dot product of X and the  theta values, which will equate to the predicted values as if they were put through y=mx+b and stored into a matrix. Following this, we compute the MSE for both sets of data and print those out, then proceeding to graph the calculated data - thus producing the first graph with the linear representation.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the improvement section, we take a slightly altered approach in implementing the normal equation, regenerating both sets of x data using the polynomial function previously created, with the goal of creating a cubic function to better fit the data. We do this by now recreating the design matrix with the updated data, and then calculating the normal equation once again. With these theta values we can now generate the updated predicted y values, and then calculate the new MSE. To graph the values accurately with the adjustments made, we then need to create a new array called x_plot containing 100 evenly spaced values between the min and maximum x values provided in xtrain. The purpose of this range of values is to plot the new polynomial with the smoothest fit that we can get over the range of already plotted training data. With a more dense set of evenly space3d values, we can compute and plot the corresponding y values in order to generate a smooth curve - thus more accurately representing the data. Once generated, we can then finish by plotting the test and train data, with the polynomial line fitting the data as shown in the output.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Z7Sjv2rbvm"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "Imagine that you are a realtor in Auburn. You have data points (See excel file. Last column is the\n",
        "target variable.) that correspond to the recent sales of different houses in and around Auburn. Your\n",
        "goal is to help estimate the prices of houses that one can use to sell or buy listings. Can you use your\n",
        "knowledge of linear regression to find the best regression model? Use your implementation from\n",
        "Question 1 (without any basis functions) to answer the following questions.\n",
        "1. What is the average least squares error for the given data using your simple linear regression\n",
        "model?\n",
        "2. Which feature has the greatest impact on the predicted house prices? How can you justify\n",
        "this finding? Could this feature alone serve as a predictor for house prices?\n",
        "3. Which feature has the least impact on the predicted house prices? How can you verify\n",
        "this? What is the impact on model performance when this feature is removed from the\n",
        "dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AHc8uR0BeEl-",
        "outputId": "f3387b36-a379-4ac7-e8bb-517bc0b58576"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Mean Squared Error calculation function.\n",
        "def mse(true_values, predicted_values):\n",
        "    return np.mean((true_values - predicted_values) ** 2)\n",
        "\n",
        "def normal_equation(X, y):\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "# Loading in the housing data from google drive.\n",
        "# house_data = pd.read_excel('/content/drive/MyDrive/COMP-5630/Housing_data_regression.xlsx')\n",
        "\n",
        "# Loading in the housing data locally.\n",
        "house_data = pd.read_excel('Housing_data_regression.xlsx')\n",
        "\n",
        "features = house_data.columns[1:-1]\n",
        "print(\"Features:\", features)\n",
        "print('\\n')\n",
        "\n",
        "# Store the target values from the price column.\n",
        "y_targets = house_data['Price'].values\n",
        "\n",
        "# Store the design matrix of the features with the added 1s column.\n",
        "X_feature = house_data[features].values\n",
        "\n",
        "X_feature = np.c_[np.ones(X_feature.shape[0]), X_feature]\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the theta values for each feature using normal equation.\n",
        "theta_vals = normal_equation(X_feature, y_targets)\n",
        "\n",
        "# Utilize these theta vals to predict y values with the regression model.\n",
        "y_pred = X_feature @ theta_vals\n",
        "\n",
        "\n",
        "# Calculate the MSE to find the average least squares error.\n",
        "mse_val = mse(y_targets, y_pred)\n",
        "print(\"Average Least Squares Error:\", mse_val)\n",
        "\n",
        "\n",
        "# By taking the absolute value of all of the theta values,\n",
        " # (aside form the 1s column), we can accurately\n",
        "feat_impact = np.abs(theta_vals[1:])\n",
        "most_impactful = np.argmax(feat_impact)\n",
        "least_impactful = np.argmin(feat_impact)\n",
        "\n",
        "print(\"Feature with most impact:\", features[most_impactful])\n",
        "print(\"Feature with least impact:\", features[least_impactful])\n",
        "print('\\n')\n",
        "\n",
        "###########################################################\n",
        "#        Retrain model with only this feature.            #\n",
        "###########################################################\n",
        "\n",
        "top_feature = house_data[features[most_impactful]].values\n",
        "\n",
        "top_feature = np.c_[np.ones(top_feature.shape[0]), top_feature]\n",
        "\n",
        "top_theta = normal_equation(top_feature, y_targets)\n",
        "\n",
        "top_ypred = top_feature @ top_theta\n",
        "\n",
        "top_mse = mse(y_targets, top_ypred)\n",
        "print(\"Top Feature Only Most Squares Error:\", top_mse)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "###########################################################\n",
        "#     Retrain model without least impactful feature.      #\n",
        "###########################################################\n",
        "\n",
        "# Drop the least impactful feature.\n",
        "updated_features = features.drop(features[least_impactful])\n",
        "\n",
        "# Create an updated design matrix without the least impactful feature.\n",
        "updated_X = house_data[updated_features].values\n",
        "updated_X= np.c_[np.ones(updated_X.shape[0]), updated_X]\n",
        "\n",
        "# Calculate the new optimal params using the normal equation.\n",
        "updated_theta = normal_equation(updated_X, y_targets)\n",
        "\n",
        "updated_ypred = updated_X @ updated_theta\n",
        "\n",
        "updated_mse = mse(y_targets, updated_ypred)\n",
        "print(\"New Average Least Squares Error:\", updated_mse)\n",
        "\n",
        "\n",
        "feat_impact_updated = np.abs(updated_theta[1:])\n",
        "most_impactful_updated = np.argmax(feat_impact_updated)\n",
        "least_impactful_updated = np.argmin(feat_impact_updated)\n",
        "\n",
        "print(\"Updated Feature with most impact:\", updated_features[most_impactful_updated])\n",
        "print(\"Updated Feature with least impact:\", updated_features[least_impactful_updated])\n",
        "print('\\n')\n",
        "\n",
        "house_data.head(20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjgTr9mieV-P"
      },
      "source": [
        "# Question 2 Report\n",
        "\n",
        "Regarding the approach and thought process I had for this question, the first thing I did was import the data from the spreadsheet, and print the head of the dataframe to nicely show each row and its corresponding column data. From here I was able to build a solid understanding of what the raw data looked like, and how I could break it up to be manipulated.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Next, I began to separate the data in an organized matter in order to accomplish the goal. I first took just the final column of the table to isolate the 'target values', or what we are looking for the true outcome to be. With the same intuition, I also created an array called features in order to isolate the specific features from the column containing the index values, as well as from the target value column. From here I determined I was ready to move forward with creating the linear regression using the normal equation.\n",
        "\n",
        "<br/>\n",
        "\n",
        "This time around, I decided to make my life a little easier by creating a short function called normal_equation(X, y), where the input parameter X would be the design matrix for the features, and y would be the target values. Through this calculation the theta optimized values are produced, allowing us to continue to produce prediction values given the data. Upon calculating prediction values, we can then calculate the initial MSE, which proved to be a large value, (over 5 billion).\n",
        "\n",
        "<br/>\n",
        "\n",
        "In order to attempt to refine the model with a goal of decreasing the error and increasing the likelihood for an accurate prediction, we identify the most impactful feature, which would have the largest correlation / weight value, (from our theta optimized calculations), as well as the least impactful feature, which would have the smallest correlation / weight value. Upon identifying these, we can drop the least impactful feature column from consideration and recalculate the theta values as well as the predicted y, in order to find a new MSE.\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "Interestingly enough, removing the least impactful feature resulted in a larger MSE, which normally would seem to indicate the removal of a more impactful feature; however the increase did not seem to be absurdly large, so it is possible that the feature really did not have as big of an impact, or that the method of calculation could use some refining.\n",
        "\n",
        "\n",
        "## Sub-questions:\n",
        "\n",
        "1. The Average least squares error for the given data using the simple linear regression model came out to 5006561505.204384, which did in fact seem quite large to begin with. However, when taking into account that these values being considered for price are in the hundreds of thousands, (Ranging anywhere from the \\$390,000-$1,640,000 range), the squaring of some of these value differences seem far more understandable.\n",
        "\n",
        "<br />\n",
        "\n",
        "2. The feature that served as the most impactful after the first attempt, was the living area. We were able to verify this by identifying its correlation / weight value to be the largest amongst all features. Upon calculating what the MSE would be with only this feature in consideration, the value was significantly higher than that of the model considering all features, indicating that this model could not be represented accurately with only the one feature.\n",
        "\n",
        "<br />\n",
        "\n",
        "3. The feature that served as teh least impactful after the first attempt was the age of the home. This was indicated by it having the smallest correlation / weight value calculated, meaning that computationally speaking it has the smallest impact on the predicted price value that was outputted. Upon recalculating the results with this feature removed, it was found that oddly enough the MSE increased slightly, (in comparison to the trial that only included the single feature), indicating that this feature does in fact play a relatively minimal role in the impact of these features on house prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQQM9N-plSg1"
      },
      "source": [
        "# Question 3\n",
        "\n",
        "Suppose you are working on a binary classification problem where you are tasked with predicting whether a certain type of tumor is malignant (y = 1) or benign (y = 0) based on a set of features. You are given a dataset that includes these features and corresponding labels for training.\n",
        "\n",
        "<br>\n",
        "\n",
        "Your goal is to implement a logistic regression model from scratch (without using any pre-packaged libraries such as Sci-Kit Learn) to classify the tumors. You should use gradient descent to find the optimal parameters that minimize the cost function. Here are some tasks to guide you through this problem: [Code Implementation 11 points]\n",
        "\n",
        "<br/>\n",
        "\n",
        "1. Implement the Sigmoid Function: Write the code for the sigmoid function that will map your\n",
        "predictions to probabilities.\n",
        "\n",
        "<br/>\n",
        "\n",
        "2. Derive the Cost Function: Formulate the cost function for logistic regression and explain why it is different from the cost function in linear regression.\n",
        "\n",
        "<br/>\n",
        "\n",
        "3. Apply Gradient Descent: Implement the gradient descent algorithm to minimize the cost function and optimize the parameters of your model. Ensure that you include a stopping\n",
        "criterion based on either a fixed number of iterations or a threshold for changes in the cost function.\n",
        "\n",
        "<br/>\n",
        "\n",
        "4. Evaluate the Model: Once the model is trained, evaluate its performance on a test dataset. Plot the decision boundary and explain how the logistic regression model separates the two classes.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Answer the following questions: [9 points]\n",
        "1. Why is the sigmoid function critical in logistic regression? How does it influence the interpretation of the output?\n",
        "\n",
        "<br/>\n",
        "\n",
        "2. What are the key differences between the logistic regression model and the linear regression model, particularly in the context of the cost function and the type of problem each is used for?\n",
        "\n",
        "<br/>\n",
        "\n",
        "3. How would you modify your logistic regression model to handle multi-class classification problems?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIl-Ku2nBgtF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "cabMpItzCtI4",
        "outputId": "6d902894-51ff-4f6c-904e-b26b22e2cfed"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Defines a sigmoid function to easily compute the mathematical value\n",
        "# for the entire input matrix.\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Defines a cost function calculator to easily reuse without having to rewrite\n",
        "# the logic each time.\n",
        "def cost_function(X, y, theta):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X @ theta)\n",
        "    cost = (-1 / m) * (np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)))\n",
        "    return cost\n",
        "\n",
        "# Initializes theta to zeros for a better starting point\n",
        "def theta_init(X):\n",
        "    return np.zeros((X.shape[1], 1))\n",
        "\n",
        "# Adds a column of ones to X for the bias term.\n",
        "def adjustX(X):\n",
        "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "# Defines a function for calculating gradient descent to optimize theta.\n",
        "def gradient_descent(X, y, theta, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        h = sigmoid(X @ theta)\n",
        "        gradient = (1 / m) * (X.T @ (h - y))\n",
        "        theta -= learning_rate * gradient\n",
        "        cost = cost_function(X, y, theta)\n",
        "        costs.append(cost)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
        "\n",
        "    return theta, costs\n",
        "\n",
        "# Import the data from google drive.\n",
        "# tumor_test = pd.read_csv('/content/drive/MyDrive/COMP-5630/Question3_Tumor_test_data.csv')\n",
        "# tumor_train = pd.read_csv('/content/drive/MyDrive/COMP-5630/Question3_Tumor_train_data.csv')\n",
        "\n",
        "# Import the data locally.\n",
        "tumor_test = pd.read_csv('Question3_Tumor_test_data.csv')\n",
        "tumor_train = pd.read_csv('Question3_Tumor_train_data.csv')\n",
        "\n",
        "# Initialize parameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Feature and label extraction\n",
        "test_features = tumor_test.columns[:-1]\n",
        "train_features = tumor_train.columns[:-1]\n",
        "\n",
        "X_test = tumor_test[test_features].values\n",
        "y_test = tumor_test['Malignancy (y)'].values\n",
        "\n",
        "X_train = tumor_train[train_features].values\n",
        "y_train = tumor_train['Malignancy (y)'].values\n",
        "\n",
        "# Account for the bias term by adding a column of 1s.\n",
        "X_test = adjustX(X_test)\n",
        "X_train = adjustX(X_train)\n",
        "\n",
        "# Initialize theta\n",
        "curr_theta = theta_init(X_train)\n",
        "\n",
        "# Train the model using gradient descent\n",
        "curr_theta, costs = gradient_descent(X_train, y_train.reshape(-1, 1), curr_theta, learning_rate, num_iterations)\n",
        "\n",
        "print('Final parameters (thetas):', curr_theta.flatten())\n",
        "print('Final cost post training', costs[-1])\n",
        "\n",
        "# Plotting decision boundary\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train[:, 1], X_train[:, 2], c=y_train.flatten(), cmap='viridis', edgecolors='k')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "x_vals = np.linspace(X_train[:, 1].min(), X_train[:, 1].max(), 100)\n",
        "y_vals = -(curr_theta[0] + curr_theta[1] * x_vals) / curr_theta[2]\n",
        "plt.plot(x_vals, y_vals, label='Decision Boundary', color='red')\n",
        "plt.legend()\n",
        "plt.title('Logistic Regression Decision Boundary')\n",
        "# Adjusting y-axis limits\n",
        "plt.ylim([X_train[:, 2].min() - 1, X_train[:, 2].max() + 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COnC6VbFD3Gm"
      },
      "source": [
        "# Question 3 Report:\n",
        "\n",
        "My intution going into this question was to try and organize each calculation into its own function, as I knew that gradient descent would involve iterations. At first, pulling and organizing the data for processing was simple, and resembled heavily what had been done in the last two questions. The difficult part, came wiht organizing the math in a way that was understandable within the gradient descent function, and then for the decision boundary calculation which I clearly was not able to do correctly. The gradient descent would at different points produce a very unclear matrix of theta values, which I initially attempted to populate with random values, but continued to run into issues. Eventually with the help of hours of research and viewing different examples of how others have implemented gradient descent, I was able to piece together what I feel is an example that at least scratches the surface through its ability to calculate the end values for the parameter vector, thus showing the affects of each feature, as well as displaying the trend in how the cost was changing as the script continued through each iteration.\n",
        "\n",
        "<br>\n",
        "\n",
        "I have a goal moving forward to better understand the nitty gritty of each piece of math being done here, and the reasoning behind it. I have began to do my own research to better understand each piece, however I think I may need to start fresh on a similar example to better gauge my understanding of how to properly apply gradient ascent from scratch.\n",
        "\n",
        "1. The sigmoid function can be considered critical to lgoistic regression due to the fact that it maps the predictions to probabilities, which help to satisfy the criteria for a binary classification. Since the sigmoid function 'squashes' the data, and is able to effectively display the relationship in terms of approaching 1, 0, and 0.5, we can thus clearly see how the prediction should trend.\n",
        "\n",
        "<br>\n",
        "\n",
        "2. The key differences between linear and logistic regression are first and formost the outputs being evaluated, as linear regressions are utilized for continuous outputs, such as house prices, whereas logistic regression is used for binary classifications, where there is either one outcome or another, (1, 0). For a linear regression, the relationship is also modelled as a linear combination of input features, attempting to minimize the squared residuals for each point of data. For Logistic regression, however, the probability of class membership, (belonging to one group or the other), is modeled through a logistic function, which minimize the cost each iteration based on the log-likelihood.\n",
        "\n",
        "<br>\n",
        "\n",
        "3. After exploring possible ways to modify this regression to handle multiclass classification, I came across the usage of the softmax function, which can be used in place of the sigmoid function. In this case, each class would have its own regression formula, helping to predict a probability distribution over the several different class labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1EGYhhpID6jC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
