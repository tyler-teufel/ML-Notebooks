{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "This notebook was created by Tyler Teufel, and is broken down by each question, containing all code required as well as the individual reports for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "**Question:** \n",
    "\n",
    "*Derive the update rule and show how to train a 2-layer (1 hidden layer and 1 output layer) neural network with backpropagation for  regression using the Mean Square Error loss. Assume that you are  using the Sigmoid activation function for the hidden layer. Explain briefly how this is different from the update rule for the network trained for binary classification using log loss.*\n",
    "\n",
    "\n",
    "### Report:\n",
    "\n",
    "In order to derive the update rule for the neural network, lets first consider its structure. The neural network that is to be trained can be broken down to just one hidden layer, and one output layer, making up its 2-layer structure. The input layer will take in the respective features from the feature vector, and send them to the hidden layer where the weighted sum of the input features is taken, followed by the application of the sigmoid application function to this weighted sum at the neuron such that it can be correctly mapped to the output layer once the data has been squashed down to a value between 0 and 1. Once the transformed data has been transformed, it becomes the 'input' terms for the output layer. These values are then used to calculate a linear combinated given either the randomly generated weights and biases for the output layers, or the learned weights and biases, depending on if this is the first instance of forward pass or not. Once completed, the outputed value from the linear combination, is the predicted value that our neural network has produced. This can then be used to compute the mean squared error by utilizing the function \n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\frac{1}{2} (y - \\hat{y})^2\n",
    "\\]\n",
    "\n",
    "With this value, we can now pivot to the derivation of the update rule, for training the neural network with backpropagation for this regression. The first step is to compute the gradient of the loss, with respect to the prediction. For calculating the gradient, we take the derivative of the mean squared error loss function:\n",
    "\n",
    "\\[\n",
    "  \\frac{dJ}{d\\hat{y}} = \\frac{d}{d\\hat{y}}(\\frac{1}{2}(y - \\hat{y})^2) = -(y - \\hat{y}) \n",
    "\\]\n",
    "\n",
    "By computing the derivative, we have essentially found the rate of change for MSE loss, which is our gradient for the loss function. The next step will be to compute the gradients with respect to the weight and with respect to the bias, respectively, for the output layer such that they can be adjusted accordingly in the training process.\n",
    "\n",
    "The next step is to move onto the hidden layer, where we begin by taking the gradient of the loss function with respect to the hidden layer output. Lets represent the hidden layer output with \\(a_h\\), and the gradient as \\(\\frac{\\partial J}{\\partial a_h}\\), which can be computed using the chain rule by finding the product of the gradient of the loss function with respect to the predicted value, and the gradient of the predicted value with respect to the hidden layer output value.\n",
    "\n",
    " You can start to see the pattern here- we are continuing to propogate back through each layer and compute the the gradients for each function at each layer, thus improving and training the data. This process is repeated continually using the chain rule, taking the gradient of the loss function with respect to the hidden layers weighted input value to the activation function, and then the weights and biases here. Through calculating all of these gradients, we can very easily plug in for each step of the update rules for each layers weights and biases:\n",
    "\n",
    " \\[\n",
    "w_i := w_i - \\alpha \\frac{\\partial J}{\\partial w_i}\n",
    "\\]\n",
    "\n",
    "where \\(w_i\\) represents the weight i to be updated at the layer  that we are updating. This backwards pass through the neural network finds the gradients of each function with respect to its input, and calculates how it should be adjusted such that we can identify the relationship between the data being studied. \n",
    "\n",
    "### How is this different from the network trained for binary classification with log loss?\n",
    "\n",
    "The biggest difference to identify here is that, since we are using a regular regression in the initial network, there is no activation function being computed on the output layer, so the produced predicted value is a continuous value, representing an actual value as opposed to a binary classification, where the output is literally classifying the input as something. \n",
    "\n",
    "With this in mind, we also are using a different loss function, which will produce different gradients. the MSE loss function is computing the difference between the predicted value and the output value, whereas the log loss function is computing the difference between the predicted probability / classification, and the true classification. We are essentially diverting between comparing continuous values in one instance, and probabilitiies in another. On one hand, we are updating based upon the magnitude of error, whereas on the other hand we are updating based upon probability differences, leading to differing behaviors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "For the given data on  Canvas, construct a neural network for the regression task. Your network must have 1 hidden  layer  and  1  output  layer.  Use  sigmoid  to  be  your  activation  function  for  the  hidden  layer(s).  You  can choose the number of neurons in each layer using your intuition. \n",
    " \n",
    "The data is already split to have your input data for training (X_train.csv) and testing (X_train.csv) and their corresponding target values Y_train.csv and Y_test.csv, respectively. You can load the data using NumPy as follows: \n",
    " \n",
    "```python\n",
    "X_train = np.loadtxt(\"X_train.csv\") \n",
    "```\n",
    " \n",
    "Implement the backpropagation algorithm and train your network until convergence. \n",
    " \n",
    "Answer the following questions: \n",
    "1. What is the activation function that you will choose for the output layer? Justify your answer briefly. \n",
    "\n",
    "<br>\n",
    "\n",
    "2. How many neurons should there be in the output layer? Why? \n",
    "\n",
    "<br>\n",
    "\n",
    "3. Report the average MSE loss and the accuracy. \n",
    "\n",
    "<br>\n",
    "\n",
    "4. Plot the loss and accuracy as a function of the number of iterations. \n",
    "\n",
    "<br>\n",
    "\n",
    "5. What is the effect of the learning rate on the training process? Vary the learning rate to be between 0.001 and 1.0 and plot the resulting accuracy as a function of learning rate. \n",
    "\n",
    "<br>\n",
    "\n",
    "6. What is the effect of the number of neurons in the hidden layer? To answer this question, you will need to consider and answer the following: \n",
    "   <br>\n",
    "   \n",
    "   a. You  will  need  to  vary  the  number  of  neurons  from  1  to  10.  Does  the  update  rule  need  to  be changed/derived again? Why or why not? \n",
    "   \n",
    "   <br>\n",
    "\n",
    "   b. Report your observations by reporting the final loss and plotting the true labels and your predicted labels, along with a brief (2-3 lines) description. \n",
    "\n",
    "<br>\n",
    "\n",
    "7. What is the  effect of the activation functions in the  network?  Explore two different  activation functions other than sigmoid such as tanh, linear, or ReLU. \n",
    "<br>\n",
    "    a. Will you need to change the update rule? \n",
    "\n",
    "    <br>\n",
    "\n",
    "    b. What is the change that you need to make to achieve this experiment? \n",
    "\n",
    "    <br>\n",
    "\n",
    "    c. Report your observations by reporting the final loss and plotting the true labels and your predicted labels, along with a brief (2-3 lines) description. \n",
    "\n",
    "<br>\n",
    "\n",
    "8. Split the training data into training and validation set and apply early stopping criteria. \n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "    a. How the training and validation loss changes as you change the “patience” in early stopping? \n",
    "<br>\n",
    "\n",
    "    b. Plot the training vs. validation loss curves. Justify whether your model overfits or underfits as the patience changes. \n",
    "\n",
    "<br>\n",
    "\n",
    "9. Implement another regularization technique for neural network as discussed in the class. \n",
    "\n",
    "    Compare and contrast with early stopping and your chosen regularization technique. \n",
    "Which one would you prefer for this dataset setting? Justify your answer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pipx-jupyter)",
   "language": "python",
   "name": "pipx-jupyter"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
