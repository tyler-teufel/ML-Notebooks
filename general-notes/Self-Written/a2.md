## Question 1

**Question:** 

*Derive the update rule and show how to train a 2-layer (1 hidden layer and 1 output layer) neural network with 
backpropagation  for  regression  using  the  Mean  Square  Error  loss.  Assume  that  you  are  using  the  Sigmoid 
activation function for the hidden layer. Explain briefly how this is different from the update rule for the network 
trained for binary classification using log loss.*


### Report:

In order to derive the update rule for the neural network, lets first consider its structure. The neural network that is to be trained can be broken down to just one hidden layer, and one output layer, making up its 2-layer structure. The input layer will take in the respective features from the feature vector, and send them to the hidden layer where the weighted sum of the input features is taken, followed by the application of the sigmoid application function to this weighted sum at the neuron such that it can be correctly mapped to the output layer once the data has been squashed down to a value between 0 and 1. Once the transformed data has been transformed, it becomes the 'input' terms for the output layer. These values are then used to calculate a linear combinated given either the randomly generated weights and biases for the output layers, or the learned weights and biases, depending on if this is the first instance of forward pass or not. Once completed, the outputed value from the linear combination, is the predicted value that our neural network has produced. This can then be used to compute the mean squared error by utilizing the function 

\[
J(\theta) = \frac{1}{2} (y - \hat{y})^2
\]

With this value, we can now pivot to the derivation of the update rule, for training the neural network with backpropagation for this regression. The first step is to compute the gradient of the loss, with respect to the prediction. For calculating the gradient, we take the derivative of the mean squared error loss function:

\[
    
\]
